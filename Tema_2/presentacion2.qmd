---
title: 'Tema 2: Análisis multivariado de series temporales^(2)^'
subtitle: '[Curso: Tópicos Avanzados de Series Temporales](https://shuwei325.github.io/SP2600-I25)'
format: clean-revealjs
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Shu Wei Chou Chen
    url: https://shuwei325.github.io
    orcid: 0000-0001-5495-2486
    email: shuwei.chou@ucr.ac.cr
    affiliations: 
    - name: Escuela de Estadística, Universidad de Costa Rica
      url: https://www.estadistica.ucr.ac.cr/
#date: last-modified
lang: es  # español
filters:
  - reveal-auto-agenda
auto-agenda:
  bullets: numbered
  heading: Contenido
bibliography: references.bib
bibliographystyle: apa
---

```{r}
library(ggplot2)
library(tidyverse)
library(astsa)
library(MTS)
```



# VARMA(p,q)
 
## VARMA(p,q)

- Conocido como VARMA o MARMA.
- ARMA(p,q) vectorial de $k$ dimensiones se define como

$$X_{t}=\phi_0+ \sum_{i=1}^p \phi_i X_{t-i}  - \sum_{j=1}^q \theta_j {a}_{t-j} +a_{t}$$
con $\phi_p, \theta_q \neq 0$ y $\Sigma_w$ definida positiva.

- Los coeficientes $\phi_i :i=1,...,p$, $\theta_j:j=1,...,q$ son matrices $k \times k$ y $a_t$ es ruido blanco.

 
---

- Si este modelo es estacionario: 
$$X_t-\sum_{j=1}^p \phi_j X_{t-j}=\phi_0- \sum_{j=1}^q \theta_j {a}_{t-j} +a_t$$

- Aplicamos la esperanza en ambos lados, se obtiene: $\mu- \sum\limits_{j=1}^p \phi_j \mu=\phi_0$
- Sustituimos $\phi_0$ en la ecuación anterior 

$$X_t-\sum_{j=1}^p \phi_j X_{t-j}=\mu- \sum_{j=1}^p \phi_j \mu - \sum_{j=1}^q \theta_j {a}_{t-j} +a_t$$
$$\Rightarrow  
(X_t-\mu)-\sum_{j=1}^p \phi_j (X_{t-j}-\mu)=- \sum_{j=1}^q \theta_j {a}_{t-j} +a_t$$

---

- De esta forma, podemos reescribir el modelo centrado como:

$$(X_t-\mu)-\sum_{j=1}^p \phi_j (X_{t-j}-\mu)=- \sum_{j=1}^q \theta_j {a}_{t-j} +a_t$$

o bien,

$$\tilde{X_t}-\sum_{j=1}^p \phi_j \tilde{X}_{t-j}= a_t - \sum_{j=1}^q \theta_j {a}_{t-j}$$
donde $\tilde{X_t}=X_t-\mu$.

---

- Entonces, si un VARMA es estacionario se puede expresar su versión centrada, o bien sin pérdida de generalidad suponer que $\phi_0=0$.

- Además, su representación es similar al caso univariado

$$\phi(B) X_{t}= \theta(B) a_{t}$$
en donde

$\phi(B)=I- \phi_1 B-...- \phi_p B^p$ es el [operador autorregresivo]{.alert} y

$\theta(B)=I-\theta_1 B-...- \theta_q B^q$ es el [operador de medias móviles]{.alert}.


# VAR(p)

## VAR(p)

- El modelo VAR(p):
$$X_{t}=\phi_0+\phi_1 X_{t-1}+...+\phi_pX_{t-p}+a_{t}$$
donde $\phi_0$ es un vector de dimensión $k$, $\phi_i$ matrices $k \times k$ para $i=1,...,p$, $\phi_p \neq 0$ y $a_t$ es una secuencia de i.i.d. vectores aleatorios con media 0 y matriz de covariancias $\Sigma_a$, que es definida positiva.


- Su representación con el operador autorregresivo.
$$\phi(B) X_{t}=\phi_0+a_{t}$$
donde $\phi(B)=I_k- \phi_1 B-...- \phi_p B^p$ es el operador autorregresivo.

- De la misma forma, se puede considerar su versión centrada (si es estacionario):
$$\phi(B) \tilde{X}_{t}= a_{t}$$

## VAR(1)

- Considere el modelo VAR(1) con $k=2$:

$$X_{t}=\phi_0+\phi_1 X_{t-1} + a_{t}$$

- Expresando explícitamente la ecuación,

$$\begin{bmatrix}X_{1,t}\\
X_{2,t}
\end{bmatrix} =\begin{bmatrix}\phi_{10}\\
\phi_{20} \end{bmatrix} + \begin{bmatrix}\phi_{1,11} & \phi_{1,12} \\
\phi_{1,21} & \phi_{1,22}
\end{bmatrix} \begin{bmatrix}X_{1,t-1}\\
X_{2,t-1}
\end{bmatrix}  +\begin{bmatrix}a_{1,t}\\
a_{2,t}
\end{bmatrix}$$

- o bien,

$$X_{1,t}=\phi_{10}+\phi_{1,11}X_{1,t-1}+\phi_{1,12}X_{2,t-1}+a_{1,t}$$

$$X_{2,t}=\phi_{20}+\phi_{1,21}X_{1,t-1}+\phi_{1,22}X_{2,t-1}+a_{2,t}$$

---

- Suponga que $\phi_{1,12}=\phi_{1,21}=0$, entonces

$$\begin{bmatrix}X_{1,t}\\
X_{2,t}
\end{bmatrix} =\begin{bmatrix}\phi_{10}\\
\phi_{20} \end{bmatrix} + \begin{bmatrix}\phi_{1,11} & 0 \\
0 & \phi_{1,22}
\end{bmatrix} \begin{bmatrix}X_{1,t-1}\\
X_{2,t-1}
\end{bmatrix}  +\begin{bmatrix}a_{1,t}\\
a_{2,t}
\end{bmatrix}.$$

- Evaluando cada ecuación:


$$X_{t,1}=\phi_{10}+\phi_{1,11}X_{1,t-1}+a_{t,1}$$

$$X_{t,2}=\phi_{20}+\phi_{1,22}X_{2,t-1}+a_{t,2}$$

- Tenemos simplemente dos procesos de AR(1).

 
---

- Suponga que $\phi_{1,12}=0$ y $\phi_{1,21} \neq 0$, entonces

$$\begin{bmatrix}X_{1,t}\\
X_{2,t}
\end{bmatrix} =\begin{bmatrix}\phi_{10}\\
\phi_{20} \end{bmatrix} + \begin{bmatrix}\phi_{1,11} & 0 \\
\phi_{1,21} & \phi_{1,22}
\end{bmatrix} \begin{bmatrix}X_{1,t-1}\\
X_{2,t-1}
\end{bmatrix}  +\begin{bmatrix}a_{1,t}\\
a_{2,t}
\end{bmatrix}$$

- Evaluando cada ecuación:


$$\begin{align}
X_{1,t}=\phi_{10}+\phi_{1,11}X_{1,t-1}+a_{1,t} \\
X_{2,t}=\phi_{20}+\phi_{1,21}X_{1,t-1}+\phi_{1,22}X_{2,t-1}+a_{2,t}
\end{align}$$

- Tenemos que $X_{1,t}$ no depende de los valores pasados de $X_{2,t}$, pero $X_{2,t}$ depende de los valores pasados de $X_{1,t}$. Se dice que $X_{1,t}$ y $X_{2,t}$ tienen [una relación de función de transferencia]{.alert}.

- En econometría, se dice que $X_{t,1}$ [causa]{.alert} a $X_{t,2}$ en el sentido de [Granger]{.alert}, pero inversamente no: $X_{t,2}$ [no causa]{.alert} a $X_{t,1}$ en el sentido de [Granger]{.alert}.

# Causalidad de Granger 
 
## Causalidad de Granger

- El concepto de causalidad de Granger fue introducido por @Granger1969 considerando una serie bivariada $(X_{1,t,},X_{2,t})$ y el ajuste de un modelo VAR y los modelos univariados para realizar predicción. 
- Se dice que $X_{1,t}$ causa a $X_{2,t}$ en el sentido de Granger, si la predicción usando modelo VAR es más precisa que la predicción utilizando el modelo univariado.
- Formalmente, sean: 
  - $F_t$ toda la información disponible hasta el tiempo t (inclusive),
  - $F_{-i,t}$ como toda la información $F_t$ pero sin el $i-$ésimo componente $X_{i,t}$.
- En término del VAR(1) bidimensional descrito anteriormente,
  - $F_t=\left\lbrace X_t,X_{t-1},X_{t-2},... \right\rbrace$
  - $F_{-1,t}=\left\lbrace X_{2,t},X_{2,t-1},X_{2,t-2},... \right\rbrace$
  - $F_{-2,t}=\left\lbrace X_{1,t},X_{1,t-1},X_{1,t-2},... \right\rbrace$
  
---

- Sean:
  - $X_{j,t+h}|F_{t}$, $j=1,2$ como las predicciones $h$ pasos adelante basado en toda la información disponible $F_{t}$ y $e_{j,t+h}|F_{t}$ sus errores de pronósticos respectivos.
  - $X_{j,t+h}|F_{-i,t}$, $j=1,2$ como las predicciones $h$ pasos adelante basado en $F_{-i,t}$ y $e_{j,t+h}|F_{-i,t}$ sus errores de pronósticos respectivos.
- Se dice que que $X_{1,t}$ [causa]{.alert} a $X_{2,t}$ [en el sentido de Granger]{.alert}, si 

$$Var\left(e_{2,t+h}|F_{t}\right)< Var\left(e_{2,t+h}|F_{-1,t}\right)$$

- Retomando el VAR(1), con

$$\begin{bmatrix}X_{1,t}\\
X_{2,t}
\end{bmatrix} =\begin{bmatrix}\phi_{10}\\
\phi_{20} \end{bmatrix} + \begin{bmatrix}\phi_{1,11} & 0 \\
\phi_{1,21} & \phi_{1,22}
\end{bmatrix} \begin{bmatrix}X_{1,t-1}\\
X_{2,t-1}
\end{bmatrix}  +\begin{bmatrix}a_{1,t}\\
a_{2,t}
\end{bmatrix}$$

- Se dice que $X_{1,t}$ causa a $X_{2,t}$ en el sentido de Granger si $\phi_{1,21} \neq 0$. Sin embargo, $X_{1,t}$ no causa a $X_{2,t}$ en el sentido de Granger.

 
## Causalidad instantánea

- Recuerden que $\Sigma_a$ no necesariamente es una matriz diagonal. 

- Si $\Sigma_a$ no es una matriz diagonal, se dice que $X_{1,t}$ y $X_{2,t}$ tienen [causalidad instantánea en sentido de Granger]{.alert}.


# Invertibilidad
 
## Invertibilidad

- Por definición VAR(p) es invertible ya que está expresada en término de sus valores pasados: 

$$X_t=c+a_t+\sum_{j=0}^\infty \pi_j X_{t-j}$$

# Estacionariedad
 
## Estacionariedad de un VAR(1)

- Empezamos con un VAR(1):

- Sin pérdida de generalidad, suponga que $\phi_0=0$,

$$\begin{eqnarray}X_t &=& \phi_1 X_{t-1}+a_t\\ 
&=& \phi_1 (\phi_1 X_{t-2}+a_{t-1})+a_t \\ 
&=& \phi_1^2 X_{t-2}+\phi_1 a_{t-1}+a_t \\
&=& \phi_1^3 X_{t-3}+ \phi_1^2 a_{t-2}+\phi_1 a_{t-1}+a_t \\
&=& \vdots \\
&=& \phi_1^J X_{t-J}+ \sum_{j=0}^{J-1} \phi_1^j a_{t-j} \end{eqnarray}$$

- Al igual que el caso univariado, para que el proceso esté bien definido, es necesario que $\phi_1^{J}\rightarrow 0$ cuando $J \rightarrow \infty$.

 
---

- La condición de $\phi_1^{J}\rightarrow 0$ cuando $J \rightarrow \infty$ significa que todos los autovalores de $\lambda_i$ de la matriz $\phi_1$ satisfacen la condición de que $\lambda_i^{J}\rightarrow 0$ cuando $J \rightarrow \infty$.

- Y esto significa que el valor absoluto de todos los autovalores $\lambda_i$ de $\phi_1$ es menor que 1.

- Recordemos que para la matriz $\phi_1$, sus autovalores son las soluciones de la ecuación:
$$|\lambda I_k - \phi_1|=0.$$
- y es posible reescrribir la condición
$$\lambda^k \left|I_k - \phi_1\frac{1}{\lambda}\right|=0.$$

- Es decir, $\left|I_k - \phi_1 x\right|=0$ donde $x=1/\lambda$. Los autovalores de $\phi_1$ son inversos de la solución de esta ecuación.

 
---

- Como consecuencia, la condición necesaria y suficiente de la estacionariedad de un VAR(1) es que las soluciones de la ecuación
$\left|I_k - \phi_1 x\right|=0$ tengan valor absoluto mayor a 1.

- Es decir, la condición de estacionariedad de un VAR(1) es que las soluciones de $|\phi(B)|=0$ estén fuera del círculo unitario.
 
## VAR(p)

- El modelo VAR(p):
$$X_{t}=\phi_0+\phi_1 X_{t-1}+...+\phi_pX_{t-p}+a_{t}$$
donde $\phi_0$ es un vector de dimensión $k$, $\phi_i$ matrices $k \times k$ para $i=1,...,p$, $\phi_p \neq 0$ y $a_t$ es una secuencia de i.i.d. vectores aleatorios con media 0 y matriz de covariancias $\Sigma_a$, que es definida positiva.


- De la misma forma, se puede considerar su versión centrada (si es estacionario):
$$\phi(B) \tilde{X}_{t}= a_{t}$$

donde $\phi(B)=I_k- \phi_1 B-...- \phi_p B^p$ es el operador autorregresivo, y $\tilde{X}_{t}={X}_{t}-\mu_X$.


## VAR(p) como representación de VAR(1)

- Sin pérdida de generalidad, asuma $\phi_0=0$. Defina $\boldsymbol{X}_t=\left(X_t',X_{t-1}',...,X_{t-p+1}' \right)'$ una serie temporal $pk-$dimensional.

- El modelo VAR(p) se puede escribir como:

$$\boldsymbol{X}_t=\Phi \boldsymbol{X}_{t-1}+\boldsymbol{b}_t,$$
donde

$$\underset{(Kp \times 1)}{\boldsymbol{X}_{t}}=
\begin{bmatrix}X_{t}\\
\vdots \\
X_{t-p+1}
\end{bmatrix},~~~\underset{(Kp \times Kp)}{\Phi}=\begin{bmatrix}\phi_{1} & \phi_2 & ... & \phi_{p-1} & \phi_{p} \\
I & 0 & ... & 0 & 0 \\
0 & I & ... & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & ... & I & 0 
\end{bmatrix}~,$$

 
## VAR(p) como representación de VAR(1)

$$\underset{(Kp \times 1)}{\boldsymbol{b}_{t}}=
\begin{bmatrix}a_{t}\\
0 \\
\vdots\\
0
\end{bmatrix}$$

- La matrix $\Phi$ es conocida como [la matriz compañera]{.alert} del polinomio autorregresivo $\phi(B)$.

 
## Condición de estacionariedad de VAR(p)

- Como consecuencia de que $\boldsymbol{X}_t$ es VAR(1), la condición de estacionariedad es 
VAR(1) es que las soluciones de la ecuación
$\left|I_{kp} - \Phi B\right|=0$ tengan valor absoluto mayor a 1.

- Utilizando resultados de matrices en bloques, se puede concluir que la condición de estacionariedad de VAR(p) es que todas las soluciones de $\left|I_{kp} - \Phi B\right|=|\phi(B)|=0$ estén fuera del círculo unitario.


 
# Estimación, selección de modelos y diagnósticos
 
## Estimación

- Existen una extensa literatura en la estimación de un VAR. 
- Los más utilizados: 
  - mínimos cuadrados, 
  - máxima verosimilitud y 
  - métodos Bayesianos.

 
## Estimación de Máxima Verosimilitud

- Asumiendo que $a_t$ es normal multivariada y suponga que $X_{1:T}=\left\lbrace X_1,...,X_T \right\rbrace$ son observaciones en el tiempo, la función de verosimlitud es:

$$\left. \begin{eqnarray}L\left(X_{(p+1):T}|X_{1:p},\beta,\Sigma_a \right) &=& \prod_{t=p+1}^T p \left( X_{t} |X_{1:(t-1)},\beta,\Sigma_a\right)\\ 
&=& \prod_{t=p+1}^T p \left( a_{t} |X_{1:(t-1)},\beta,\Sigma_a\right) \\ 
&=& \prod_{t=p+1}^T \frac{1}{(2\pi)^{k/2}|\Sigma_a|^{1/2}} \exp\left[ \frac{-1}{2}a_t'\Sigma_a^{-1} a_t \right] \\
&\propto& |\Sigma_a|^{-(T-p)/2} \exp\left[ \frac{-1}{2} \sum_{t=p+1}^{T} tr(a_t'\Sigma_a^{-1} a_t) \right] \end{eqnarray}\right.$$

---

- la función de log-verosimlitud es:
$$\left. \begin{eqnarray}\ell \left(\beta,\Sigma_a \right) &=& c- \frac{T-p}{2} \log|\Sigma_a| -\frac{1}{2} \sum_{t=p+1}^{T} tr(a_t'\Sigma_a a_t) \\ 
&=& c- \frac{T-p}{2} \log|\Sigma_a| -\frac{1}{2}tr\left(\Sigma_a^{-1} \sum_{t=p+1}^{T} a_t' a_t\right) 
\end{eqnarray}\right.$$

donde $c$ es una constante que no influye en el proceso de optimización.

- Note que las propiedades $tr(CD)=tr(DC)$ y $tr(C+D)=tr(C)+tr(D)$ son utilizadas.


---

- Se puede reescribir la función de log-verosimlitud como:
$$\ell \left(\beta,\Sigma_a \right) = c- \frac{T-p}{2} \log|\Sigma_a| -\frac{1}{2} S(\beta)$$
donde $S(\beta)$ es la cantidad a minimizar del método de mínimos cuadrados.

- No vamos a entrar a los detalles de la distribución muestral de los estimadores.


## Ejemplo

- El producto interno bruto de Reino Unido, Canadá y Estados Unidos de segundo trimestre del 1980 al segundo trimestre del 2011.
- Los datos son ajustados estacionalmente.


```{r}
#| echo: true
da=read.table("q-gdp-ukcaus.txt",header=T)
head(da)
```


---

:::: {.columns}

::: {.column width="50%"}

- Serie original

```{r}
#| echo: true
tsplot(da[,3:5])
```

:::

::: {.column width="50%"}
- Transformación logarítmica
```{r}
#| echo: true
gdp=log(da[,3:5])
tsplot(gdp)
```
:::
::::

 
---

- Suponga que $X_t$ es el valor de un activo en el tiempo $t$, el retorno en el tiempo $t$, $r_t$, es:

$$r_t=\frac{X_t-X_{t-1}}{X_{t-1}}$$

- Despejando la expresión anterior:
$$X_t=(1+r_t)X_{t-1}$$
$$\Rightarrow \ln X_t=\ln (1+r_t)+ \ln X_{t-1}$$
$$\Rightarrow \nabla \ln X_t = \ln X_t - \ln X_{t-1}=\ln (1+r_t) \approx r_t,$$
si $r_t$ son cambios porcentuales pequeños.

- A partir de aquí, llamamos $r_t$ o $\nabla \ln X_t$ retornos y dicho valor es conocido como la tasa del crecimiento porcentual.

---

- Se calculan sus retornos en porcentajes, i.e. retornos*100:

```{r}
#| echo: true
x=gdp[2:126,]-gdp[1:125,]
x=x*100
tsplot(x)
```



---


:::: {.columns}

::: {.column width="40%"}
```{r}
#| echo: true
m1=MTS::VAR(x,p=1)
```

 
:::
  
::: {.column width="60%"}

El modelo estimado es:

$$\begin{bmatrix}X_{1t}\\
X_{2t}\\
X_{3t}
\end{bmatrix} =\begin{bmatrix}0.17\\
0.12 \\ 0.28 \end{bmatrix} + \begin{bmatrix}0.43 & 0.19 &0.04 \\
0.19 & 0.25 & 0.39 \\
0.32 & 0.18 & 0.17
\end{bmatrix} \begin{bmatrix}X_{1,t-1}\\
X_{2,t-1} \\ X_{3,t-1}
\end{bmatrix}  +\begin{bmatrix}a_{1t}\\
a_{2t} \\ a_{3t}
\end{bmatrix}$$
 
$$\text{donde}~~a_t \sim N\left(0, \Sigma_a \right), ~\text{con}~ \Sigma_a=\begin{bmatrix} 0.08 & 0.02 & 0.07 \\
0.02 & 0.32 & 0.17 \\
0.07 & 0.17 & 0.39
\end{bmatrix}$$

:::
::::
 
## Selección del orden de un VAR(p)

- ¿Cómo se selecciona el orden de un VAR?

  1. Pruebas de razón de verosimilitud secuencial
  2. Criterio de información

 
## 1. Pruebas de razón de verosimilitud secuencial

- La idea básica es comparar un VAR(p) con un VAR(p-1).

- Formalmente, se plantean:
$H_0:\phi_p=0$ $H_1:\phi_p \neq 0$

- Sea $\boldsymbol{\beta}=\left[\phi_0,\phi_1,...,\phi_p \right]$ el conjunto de las matrices de coeficientes de un VAR(p), y $\Sigma_{a,p}$ la matriz de covariancias de las inovaciones del modelo.

- Bajo el supuesto de normalidad, la razón de verosimilitud es dada por

$$\Delta= \frac{\max L(\beta_{p-1},\Sigma_a)}{\max L(\beta_{p},\Sigma_a)}= \left( \frac{|\hat{\Sigma}_{a,p}|}{|\hat{\Sigma}_{a,p-1}|} \right)^{(T-p)/2}$$

 
---


- El estadístico es dado por

$$M(p)= -(T-p-0.5-kp) \ln \left( \frac{|\hat{\Sigma}_{a,p}|}{|\hat{\Sigma}_{a,p-1}|} \right)$$

- Bajo el supuesto de que $H_0$ sea cierta, $M(p) \sim \chi^2_{k^2}$.

- En la práctica, se preselecciona un rezago $L$ grande y se realiza secuencialmente probando $p=0,...,L$.


 
## 2. Criterio de información

- Debido a la complejidad de selección del orden $p$, en la práctica se usan los criterios de información: AIC, BIC o Schwarz y Hanna Quinn.

$$AIC(p)= \log |\tilde{\Sigma}_{a,p}|+\frac{2}{T}pk^2$$
$$SC(p)=BIC(p)= \log |\tilde{\Sigma}_{a,p}|+\frac{\log(T)}{T}pk^2$$

$$HQ(p)= \log |\tilde{\Sigma}_{a,p}|+\frac{2\log(\log(T))}{T}pk^2$$

donde $p$ es el orden de rezago, $\tilde{\Sigma}_{a,p}$ es la estimación de $\tilde{\Sigma}_{a*}$ de acuerdo al VAR(p).


 
## Ejemplo

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: true
m3=MTS::VARorder(x,maxp=15)
```

:::
  
::: {.column width="50%"}

```{r}
#| echo: true
CI=data.frame(order=0:15,AIC=m3$aic,BIC=m3$bic,HQ=m3$hq)
CI%>%gather(
  key = "C.Info",
  value = "value",
  AIC,BIC,HQ
) %>% ggplot() +
  geom_line( aes(x = order, y = value, group=C.Info,color=C.Info)) + theme_bw()

```


:::
::::

## Diagnósticos

- Denote $R_j$ como la matriz de correlaciones de $j$ rezago de los residuales. Se plantean las hipótesis:   
$H_0: R_1=\cdots=R_m=0$    
$H_1: R_j \neq 0$, para algún $1\leq j \leq m$

- Recuerden que [el estadístico de Portmanteau multivariado]{.alert} es la versión multivariada de la prueba de Ljung-Box.
$$Q_m=T^2 \sum_{l=1}^m \frac{1}{T-l} tr \left(\hat{R}'_l\hat{R}^{-1}_0\hat{R}_l\hat{R}^{-1}_0\right)$$

- Bajo el supuesto de $H_0$,i.e. $R_l=0,~ l>0$, y $X_t$ es distribuída normalmente, para $T$ y $m$ suficientemente grandes, el estadístico se aproxima a la distribución $\chi^2_{mk^2}$.

- Para el caso de un VAR(p) se deben corregir los grados de libertad, es decir, $\chi^2_{(m-p)k^2}$, i.e. el valor de corrección es $pk^2$.



 
## Ejemplo con VAR(1){.scrollable}

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
m1=MTS::VAR(x,p=1)
```

:::
  
::: {.column width="50%"}
- Utilizando la prueba de Portmanteau multivariado

```{r}
#| echo: true
mq(m1$residuals,lag=18,adj=9) #pk^2=1*3^2
```
:::
::::


## Ejemplo con VAR(2){.scrollable}

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
m2=MTS::VAR(x,p=2)
```

:::
  
::: {.column width="50%"}
- Utilizando la prueba de Portmanteau multivariado

```{r}
#| echo: true
mq(m2$residuals,lag=18,adj=9) #pk^2=1*3^2
```
:::
::::
 
## Comparación de residuales

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
acf(m1$residuals)
```

:::
  
::: {.column width="50%"}
```{r}
#| echo: true
acf(m2$residuals)
```
:::
::::

# VMA(q)
 
## VMA(q)

- El modelo VMA(q):
$$X_{t}=\mu+a_t -\theta_1 a_{t-1}-...-\theta_q a_{t-q}$$
donde $\mu$ es un vector de dimensión $k$ que es su media, $\theta_i$ matrices $k \times k$ para $i=1,...,q$, $\theta_q \neq 0$ y $a_t$ es una secuencia de i.i.d. vectores aleatorios con media 0 y matriz de covariancias $\Sigma_a$, que es definida positiva.


- Su representación con el operador de medias móviles.
$$X_{t}=\mu + \theta(B) a_{t}$$
donde $\theta(B)=I_k- \theta_1 B-...- \theta_q B^q$ es el operador de medias móviles.

- De la misma forma, se puede considerar su versión centrada:
$$X_{t}-\mu = \theta(B) a_{t}$$

 
## VMA(q)

- El VMA(q) siempre es estacionario pero no siempre es invertible.

- La condición de invertibilidad se deriva muy similar al caso de estacionariedad de VAR(p).

- Un VMA(q) es invertible si todas las soluciones de $|\theta(B)|=0$ estén fuera del círculo unitario.


 
## VARMA(p,q)


- El modelo VARMA(p,q):

$$\phi(B) X_{t} = \phi_0 + \theta(B) a_{t}$$
donde   
$\phi(B)=I- \phi_1 B-...- \phi_p B^p$ es el operador autorregresivo y   
$\theta(B)=I-\theta_1 B-...- \theta_q B^q$ es el operador de medias móviles.

- La condición de estacionariedad es que todas las soluciones de $|\phi(B)|=0$ estén fuera del círculo unitario.

- La condición de invertibilidad es que todas las soluciones de $|\theta(B)|=0$ estén fuera del círculo unitario.


 
## VARMA(p,q)

- El VARMA tiene problemas de identificabilidad debido a que su representación no es única. Se requieren adicionalmente dos condiciones:

  1. Si $u(B)$ es factor común por izquierda de $\phi(B)$ y $\theta(B)$, entonces $|u(B)|$ es constante no nula.

  2. El rango de la matriz conjunta $\left[ \phi_p, \theta_q \right]$ es $k$, la dimensión de la serie multivariada $X_t$.

 
## VARMA(p,q)

- Algunos paquetes de R que estiman VARMA
  - `MTS`
  - `marima`

 
## En la próxima clase

### Enfocamos en el VAR(p) y veremos con más detalles

- Causalidad de Granger
- Raíz unitarias
- Procesos cointegrados

## Paquetes en R

Para replicar los ejemplos de esta presentación, necesitan estos paquetes:

```{r}
#| echo: true
library(ggplot2)
library(tidyverse)
library(astsa)
library(MTS)
```


### Referencia

::: {#refs}
:::


