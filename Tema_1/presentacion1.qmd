---
title: 'Tema 1: Análisis espectral de series temporales^(1)^'
subtitle: '[Curso: Tópicos Avanzados de Series Temporales](https://shuwei325.github.io/SP2600-I25)'
format: clean-revealjs
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Shu Wei Chou Chen
    url: https://shuwei325.github.io
    orcid: 0000-0001-5495-2486
    email: shuwei.chou@ucr.ac.cr
    affiliations: 
    - name: Escuela de Estadística, Universidad de Costa Rica
      url: https://www.estadistica.ucr.ac.cr/
#date: last-modified
lang: es  # español
filters:
  - reveal-auto-agenda
auto-agenda:
  bullets: numbered
  heading: Contenido
---
```{r}
library(plotly)
library(latex2exp)
library(astsa)
```


# Modelos de series temporales

## Modelos de series temporales

:::: {.columns}

::: {.column width="50%"}

**Definición:** Un [proceso estocástico]{.alert} es una familia de variables aleatorias (v.a.) indexada por un conjunto $\mathcal{T}$, $$\left\lbrace X(t), ~t \in \mathcal{T} \right\rbrace$$

- Generalmente:

  - $\mathcal{T}$ puede ser $\mathbb{R}$, $\mathbb{Z}$ o $\mathbb{N}$.
  - $X(t)$ es una v.a. real.

- Para cada $t \in \mathcal{T}$, $X(t)$  es una v.a. definida sobre $\Omega$ y 

- $X(t)$ es una función de dos argumentos $X(t,\omega)$, $t \in \mathcal{T}, \omega \in \Omega$.

:::
  
::: {.column width="50%"}
```{r}
#| label: fig-proceso1
#| fig-cap: Ilustración de un proceso estocástico, como una familia de variables aleatorias.
x <- seq(0,20,0.1)
t <- seq(1,10,0.1)
fmean <- 10+5*sin(2/pi*t)
dfmean <- data.frame(t=t,x=fmean,z=0)
#plot(t,fmean)

mean1 <- fmean[which(t==4)]
mean2 <- fmean[which(t==5)]
mean3 <- fmean[which(t==9)]
sd1 <- 1.5
sd2 <- 2
sd3 <- 1
sd <- c(sd1,sd2,sd3)

z1 <- dnorm(x, mean = mean1,sd = sd1)
z2 <- dnorm(x, mean = mean2,sd = sd2)
z3 <- dnorm(x, mean = mean3,sd = sd3)

df1 <- data.frame(x = x, t = 4, z = z1)
df2 <- data.frame(x = x, t = 5, z = z2)
df3 <- data.frame(x = x, t = 9, z = z3)

vline_mean1 <- data.frame(x = mean1, t = 4, z = seq(0,dnorm(mean1, mean = mean1,sd = sd1),0.01))
vline_mean2 <- data.frame(x = mean2, t = 5, z = seq(0,dnorm(mean2, mean = mean2,sd = sd2),0.01))
vline_mean3 <- data.frame(x = mean3, t = 9, z = seq(0,dnorm(mean3, mean = mean3,sd = sd3),0.01))

fig1 <- plot_ly() %>%
  add_trace(data = df1, x = ~x, y = ~t, z = ~z, type = "scatter3d", mode = "lines",color =I("black")) %>%
  add_trace(data = df2, x = ~x, y = ~t, z = ~z, type = "scatter3d", mode = "lines",color =I("black")) %>%
  add_trace(data = df3, x = ~x, y = ~t, z = ~z, type = "scatter3d", mode = "lines",color =I("black")) %>%
  add_trace(data = dfmean, x = ~x, y = ~t, z = ~z, type = "scatter3d", mode = "lines",color =I("red")) %>% 
  add_trace(data = vline_mean1, x = ~x, y = ~t, z = ~z, type = "scatter3d", mode = "lines",color =I("red")) %>% 
  add_trace(data = vline_mean2, x = ~x, y = ~t, z = ~z, type = "scatter3d", mode = "lines",color =I("red")) %>% 
  add_trace(data = vline_mean3, x = ~x, y = ~t, z = ~z, type = "scatter3d", mode = "lines",color =I("red")) %>% 
  add_trace(x = 13, y = 1, z=0, 
            type = "scatter3d", mode="text", text = "\u03BC (t)", inherit=FALSE) %>%
  layout(scene = list(
    xaxis = list(title = list(text = "X (\u03C9, t)")),
    yaxis = list(title = "t", range = c(0, 10)),
    zaxis = list(title = list(text = "f \u2093 (x)"), range = c(0,0.5))),
  showlegend = FALSE)
fig1
```

:::
::::
 
---

:::: {.columns}

::: {.column width="50%"}

- Para cada $\omega \in \Omega$, obtenemos una función de $t$, o sea, una observación de un proceso estocástico,  [una realización]{.alert} de un proceso estocástico. 

- Denotemos por $X^{(1)}(t), X^{(2)}(t),...$.

- Para un $t$ fijo, se puede visualizar la distribución de $X(t)$, por medio de alguna técnica estadística como: histograma, cálculo de medias, variancia, etc.
:::
  
::: {.column width="50%"}

```{r}
#| fig-width: 5
#| fig-height: 4
#| label: fig-proceso2
#| fig-cap: Ilustración de un proceso estocástico, como una familia de trayectorias.
realiz1 <- rnorm(n=length(t),mean = fmean, sd = sample(sd,1) )
realiz2 <- rnorm(n=length(t),mean = fmean, sd = sample(sd,1) )
realiz3 <- rnorm(n=length(t),mean = fmean, sd = sample(sd,1) )
realiz4 <- rnorm(n=length(t),mean = fmean, sd = sample(sd,1) )
realiz5 <- rnorm(n=length(t),mean = fmean, sd = sample(sd,1) )

realiz_df <- data.frame(t = t, 
                        realiz1 = realiz1, realiz2 = realiz2,
                        realiz3 = realiz3, realiz4 = realiz4, 
                        realiz5 = realiz5, fmean = fmean)

colores <- rainbow(5)

fig2 <- plot_ly(realiz_df, x = ~t, y = ~realiz1, name = "X\u207D\u00B9\u207E (t)", 
          type = 'scatter', mode = 'lines',color =colores[1]) %>% 
  add_trace(y = ~realiz2, name = "X\u207D\u00B2\u207E (t)",line = list(color = colores[2])) %>% 
  add_trace(y = ~realiz3, name = "X\u207D\u00B3\u207E (t)",line = list(color = colores[3])) %>% 
  add_trace(y = ~realiz4, name = "X\u207D\u2074\u207E (t)",line = list(color = colores[4]))  %>%
  add_trace(y = ~realiz5, name = "X\u207D\u2075\u207E (t)",line = list(color = colores[5]))  %>% 
  add_trace(y = ~fmean, name = "\u03BC (t)" ,color =I("black"))  %>% 
  layout(
    xaxis = list(title = "t", range = c(0,10)),
    yaxis = list(title = "t", range = c(0, 20)),
    legend = list(x = 0.8, y = 0.99))
fig2
```

:::
::::
  
---

- Para $t_1,t_2,...,t_n$ arbitrarios de $\mathcal{T}$ y defina la distribución conjunta 
$$F_{X_{t_1},...,X_{t_n}}(c_{1},...,c_{n} \mid t_1,...,t_n) = P\left(X_{t_1}\leq c_{1}, X_{t_2}\leq c_{2},...,X_{t_n} \leq c_{n} \right).$$

- El proceso estocástico $\left\lbrace X(t), ~t \in \mathcal{T} \right\rbrace$ será especificado si conocemos todas las distribuciones de dimensión finita de todo $n\geq 1$.

- En este caso, se trata de la especificación de un [modelos de series temporales]{.alert}.

- Por ejemplo, 

  - Si $n=1$, conocemos todas las distribuciones unidimensionales,
  - Si $n=2$, conocemos todas las distribuciones bidimensionales,
  - ...

## Ruido blanco

- Una colección de variables aleatorias no correlacionadas, $w_t$, con media 0 y variancia $\sigma_w^2$.

- Denotado por $w_t \sim wn(0,\sigma_w^2)$.

- Si una secuencia de variables es i.i.d., i.e. $w_t \sim iid(0,\sigma_w^2)$, entonces $w_t \sim wn(0,\sigma_w^2)$. ¡El inverso no es cierto!

:::: {.columns}

::: {.column width="50%"}
- Sin embargo, si un ruido blanco es Gaussiano, entonces $w_t \overset{iid}{\sim} N(0,\sigma_w^2)$.

- Simulación de una colección de $w_t \sim N(0,1)$ con $T=500$.
:::

::: {.column width="50%"}
```{r echo=FALSE, out.width = "90%"}
w = rnorm(500,0,1) 
plot.ts(w, main="")
```
:::
::::
 
## Medidas de dependencia

- Una descripción completa de un modelo de series temporales es proporcionado por la distribución de probabilidad conjunta, i.e. dados tiempos arbitrarios $t_1,...,t_n$ para $n$ entero positivo y $c_1,...,c_n$ n valores constantes:

$$F_{t_1,...,t_n}(c_1,...,c_n)=P\left(X_1\leq c_1, X_2\leq c_2,...,X_t \leq c_t \right)$$

- Aunque esa distribución describe los datos globalmente, en la práctica, esa distribución multidimencional es dificil de conocer, por lo que restringe el análisis a los momentos bajos (media, variancia, covariancia).

- La distribución marginal en el tiempo $t$,
$$F_t(x)=P(X \leq x)$$
- La función densidad marginal en el tiempo $t$,
$$f_t(x)= \frac{\partial F_t(x)}{\partial x}.$$

 
---

- [La función de media]{.alert} para el tiempo $t$ es definida por

$$\mu_t= E(X_t)=\int_{-\infty}^\infty f_t(x)dx.$$


- [La función de autocovariancia]{.alert} es definida por

$$\gamma_X(t,s)=\gamma(t,s)= Cov(X_t,X_s)=E\left[ (X_t-\mu_t)(X_s-\mu_s) \right].$$

- Mide la dependencia lineal entre dos puntos de tiempo de la misma serie.

- [La función de variancia]{.alert} en el tiempo $t$ es definida por
$$\gamma_X(t,t)=Var(X_t)=\sigma_t^2.$$

---

- [La función de autocorrelación]{.alert} es definida por

$$\rho_X(t,s)=\frac{\gamma(t,s)}{\sqrt{\gamma(t,t)\gamma(s,s)}}$$

- En la práctica, nos interesa también estimar estas cantidades: $\mu_t$, $\sigma_t^2$, $\gamma(t,s)$.

- Si tuvieramos $m$ realizaciones, se puede calcular

$$\hat{\mu}_t= \frac{X^{(1)}(t)+X^{(2)}(t)+...+X^{(m)}(t)}{m}.$$

- Pero en la práctica solamente se tiene una trayectoria.

# Procesos estacionarios

## Procesos estacionarios

**Definición:** Un [proceso estrictamente estacionario]{.alert} es un proceso estocástico cuyo comportamiento de cada colección de valores
	$$\left\lbrace X_{t_1},X_{t_2},...,X_{t_k} \right\rbrace$$
	es idéntico a un conjunto bajo un cambio de tiempo
	$$\left\lbrace X_{t_1+h},X_{t_2+h},...,X_{t_k+h} \right\rbrace.$$
	Esto es,
	$$P\left(X_{t_1} \leq c_1,...,X_{t_k} \leq c_k \right)=P\left(X_{t_1+h}\leq c_1,...,X_{t_k+h} \leq c_k \right)$$
	para todo $k=1,2,...$, todo tiempo $t_1,...,t_k$, todas las constantes $c_1,...,c_k$ y todos los cambios de tiempo $h=0, \pm 1, \pm 2,...$.

 
---

**Definición:** un [proceso débilmente estacionario]{.alert} es un proceso con variancia finita tal que 
1. la función de la media es constante $$\mu(t):=\mu_t=E(X_t)=\mu$$

2. La función de autocovariancia depende solamente de la diferencia de dos puntos $t, t+h$
 $$\gamma(t,t+h)=Cov(X_t,X_{t+h})=Cov(X_0,X_h):=\gamma(h).$$

Consecuentemente, la [función de autocorrelación]{.alert} de un proceso estacionario es definido como

$$\rho(h)=\frac{\gamma(t,t+h)}{\sqrt{\gamma(t+h,t+h),\gamma(t,t)}}=\frac{\gamma(h)}{\gamma(0)}.$$

- En la práctica, se refiere simplemente a un [proceso estacionario de segunda orden]{.alert} o [proceso estacionario]{.alert}.

# Estimación
 
## Estimación

- Si una serie es estacionaria, la media $\mu_t=\mu$ es constante y podemos estimarla usando [la media muestral]{.alert}

$$\bar{X}=\frac{\sum\limits_{t=1}^T X_t}{T}.$$
- **Resultados teóricos:** se puede probar que
$$E\left[\bar{X}\right]=\mu.$$
$$Var\left[\bar{X}\right]=\frac{1}{T} \sum_{h=-n}^n \left(1-\frac{|h|}{T} \right) \gamma_X(h).$$

---

- [La función de autocovariancia muestral]{.alert} es definida por
$$\hat{\gamma}_X(h)=\frac{1}{T}\sum_{t=h+1}^{T} (X_{t}-\bar{X})(X_{t-h}-\bar{X}),$$
con $\hat{\gamma}_X(-h)=\hat{\gamma}_X(h)$ para $h=0,1,...,T-1$.


- Y [la función de autocorrelación muestral]{.alert} es definida por
$$\hat{\rho}_X(h)=r_X(h)=r_h=\frac{\hat{\gamma}_X(h)}{\hat{\gamma}_X(0)}$$

$$=\frac{\sum\limits_{t=h+1}^{T} (X_{t}-\bar{X})(X_{t-h}-\bar{X})}{\sum\limits_{t=1}^{T} (X_{t}-\bar{X})^2}.$$
 
---

**Propiedad:** Si $X_t$ tiene sus primeros 4 momentos finitos, y $X_t$ es ruido blanco, entonces para $T$ suficientemente grande, la función de autocorrelación $\hat{\rho}_X(h), h=1,2,..., H$ donde $H$ es un valor cualquier pero fijo, es aproximadamente normal con media cero y desviación estándar
$$\sigma_{\hat{\rho}_X(h)}=\frac{1}{\sqrt{T}}.$$

**Nota:**

- Con este resultado, si se tiene un ruido blanco, entonces se espera que con aproximadamente 95% de confianza, las $\hat{\rho}_X(h)$ deberían caer dentro del intervalo $\left( \frac{-2}{\sqrt{T}},\frac{2}{\sqrt{T}} \right)$.
puede estimar intervalos de confianza  e identificar aquellos rezagos que tienen autocorrelación significativa.
- En la práctica, se grafica los pares ordenados $(h,r_h),h=1,2,...$ para visualizar la función de aucorrelación muestral. Este gráfico se denomina [correlograma]{.alert}.


# Ejemplos
 
## Ruido blanco

:::: {.columns}

::: {.column width="50%"}
- Si $w_t \sim wn(0,\sigma_w^2)$, la función de autocorrelación es

$$\rho_w(t,s)=\left\lbrace 
\begin{aligned}
1, & & t = s \\
0, & &  t \neq s,
\end{aligned}
\right.$$

o

$$\rho_w(h)=\left\lbrace 
\begin{aligned}
1, & & h = 0 \\
0, & &  h \neq 0,
\end{aligned}
\right.$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-cap: 'La ACF estimada de una serie de 500 observaciones de $w_t \overset{\text{iid}}{\sim} N(0,1)$.'
w = rnorm(500,0,1) 
acf(w,lag.max = 10)
```
:::
::::
 
## Medias móviles

- Considere el ejemplo de medias móviles: $v_t=\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1})$
 
- La función de autocovariancia es definida por
  $$\gamma_v(t,t+h)=Cov(v_{t},v_{t+h})$$  

- Caso 1 $(h=0)$: 
$$\begin{align*}\gamma_v(t,t+0)&=Cov(v_{t},v_{t+0})=Var(v_t)\\ &=\frac{1}{9}Var(w_{t-1}+w_{t}+w_{t+1})\\
&=\frac{3}{9} Var(w_t)=\frac{1}{3} \sigma_w^2.\end{align*}$$ 

---

- Caso 2 $(h=1)$: 
$$\begin{align*}
\gamma_v(t,t+1)&=Cov(v_{t},v_{t+1}) \\
&=Cov\left[\frac{1}{3}(w_{t-1}+w_{t}+w_{t+1}),\frac{1}{3}(w_{t}+w_{t+1}+w_{t+2})\right]\\
&=\frac{1}{9}\left[ Cov(w_{t},w_{t})+Cov(w_{t+1},w_{t+1})\right]=\frac{2}{9}\sigma_w^2.
\end{align*}$$

- Caso 3 $(h=-1)$: Similarmente se obtiene $\gamma_v(t,t-1)=\frac{2}{9}\sigma_w^2$

- Caso 4 $(h=2~o~h=-2)$: 

  $$\gamma_v(t,t+2)=\gamma_v(t,t-2)=\frac{1}{9}\sigma_w^2$$
  
- Caso 5 $(h>2~o~h<-2)$: 

  $$\gamma_v(t,t+h)=0.$$

---

Entonces,
$$\gamma_w(t,t+h)=\left\lbrace 
\begin{aligned}
\frac{3}{9}\sigma_w^2, & & h = 0 \\
\frac{2}{9}\sigma_w^2, & &  |h| = 1 \\
\frac{1}{9}\sigma_w^2, & &  |h| = 2 \\
0, & &  |h| > 2,
\end{aligned}
\right.$$

 
---


:::: {.columns}

::: {.column width="50%"}

- Su función de autocorrelación es
$$\rho_v(h)=\left\lbrace 
\begin{aligned}
1, & & h = 0 \\
\frac{2}{3}, & &  |h| = 1 \\
\frac{1}{3}, & &  |h| = 2 \\
0, & &  |h| > 2,
\end{aligned}
\right.$$
:::

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-cap: "Ilustración de la ACF muestral de medias móviles de orden 3."
set.seed(1)
w = rnorm(500,0,1) 
v = stats::filter(w, sides=2, filter=rep(1/3,3)) 
v = na.omit(v)
acf(v,lag.max = 10)
```


:::

::::
 
# Proceso lineal 
 
## Proceso lineal

- Un [proceso lineal]{.alert}, $X_t$ es definido como una combinación lineal de los ruidos blancos $w_t$ y es dado por:

$$X_t=\mu+\sum_{j=-\infty}^{\infty}\psi_j w_{t-j},~~~ \sum_{j=-\infty}^{\infty}|\psi_j| < \infty.$$

- Se puede demostrar que su ACF es dada por
$$\gamma_X(h)=\sigma_w^2 \sum_{j=-\infty}^{\infty}\psi_{t+h}\psi_j,~~~ \text{para } h \geq 0$$

- Para el ejemplo anterior de medias móviles, note que es un caso particular con $\psi_0=\psi_{-1}=\psi_{1}=1/3$, y $\psi_k=0, k \neq -1,0,1.$

 
## Estacionariedad

- Concentremos en un modelo lineal con $\psi_j=0,~~ \text{para}~~j<0$, i.e.

$$X_t=\mu+\sum_{j=0}^{\infty}\psi_j w_{t-j},~~~ \sum_{j=0}^{\infty}|\psi_j| < \infty.$$

- Se puede demostrar que es [estacionario]{.alert} con:

$$E(X_t)=\mu,~~~\gamma_X(h)=\sigma_w^2 \sum_{j=0}^{\infty}\psi_{t+h}\psi_j,~~~ \text{para } h \geq 0$$

- Note que tiene forma de $MA(\infty)$ y se puede presentarlo como:

$$\tilde{X}_t=X_t-\mu={\psi}(B)w_t$$
donde $\psi(B)=1+ \psi_1 B+ \psi_2 B^2+...$ es el [operador de medias móviles]{.alert} de orden infinito.


## Invertibilidad

- El interés es expresar una serie temporal $X_t$ en término de sus valores pasados $X_{t-i}$ para $i>0$.

- Una serie temporal estacionaria $X_t$ es [invertible]{.alert} si se puede expresar como:
$$X_t=c+w_t+\sum_{j=1}^\infty \phi_j X_{t-j}$$
donde $c$ es un vector constante,
$\phi_j,~j>0$ constantes, y
$\left\lbrace w_{t}\right\rbrace$ es ruido blanco con variancia $\sigma_w^2$.

 
## Representaciones de un modelo lineal

- A partir de un modelo lineal estacionario e invertible: 
$$X_t-\sum_{j=1}^\infty \phi_j X_{t-j}=c+w_t$$

- Calculemos la esperanza en ambos lados, se obtiene: $\mu-\mu \sum\limits_{j=1}^\infty \phi_j =c$
- Sustituimos $c$ en la ecuación anterior 

$$X_t-\sum_{j=1}^\infty \phi_j X_{t-j}=\mu-\mu \sum_{j=1}^\infty \phi_j + w_t$$
$$\Rightarrow (X_t-\mu) - \left(\sum_{j=1}^\infty \phi_j X_{t-j}- \sum_{j=1}^\infty \phi_j \mu \right)= w_t$$

 
---

- Podemos reescribir el modelo como:

$$\tilde{X}_t-\sum_{j=1}^\infty \phi_j \tilde{X}_{t-j}=w_t$$
donde $\tilde{X}_t=X_t-\mu$.

- Conocida como la representación de una serie temporal centrada, y es posible representarla como:
$$\phi(B)\tilde{X}_t=w_t$$

donde $\phi(B)= 1- {\phi}_1 B-{\phi}_2 B^2-...$ es el operador autorregresivo de orden infinito.

 
---

- Recordemos que un proceso lineal estacionario tiene esta representación:

$$\tilde{X}_t={\psi}(B)w_t$$

- Y un proceso invertible:

$$\phi(B)\tilde{X}_t=w_t$$

- Insertamos las dos ecuaciones:

$$\phi(B){\psi}(B)w_t=w_t$$
o bien 

$$\left(1- \phi_1 B- \phi_2 B^2-... \right) \left(1+ \psi_1 B+ \psi_2 B^2+... \right)w_t=w_t$$
 
---


- De esta forma,
$$\left(1- \phi_1 B- \phi_2 B^2-... \right) \left(1+ \psi_1 B+ \psi_2 B^2+... \right)=1$$

- Esto implica que todos los coeficientes que acompañan a $B^i~,i>0$ debe ser cero.


$$\left. \begin{eqnarray} & \psi_1-\phi_1 & = & 0 \\ & \psi_2-\psi_1\phi_1-\phi_2 &= & 0 \\ & \psi_3-\psi_1\phi_2-\psi_2\phi_1-\phi_3 & = & 0 \\
&\vdots &=& \vdots \end{eqnarray}\right.$$

- De esta forma, se puede obtener representaciones diferentes (AR o MA) de un modelo lineal.

 
## Ejemplo

- Considere un modelo AR(1): $\phi(B)\tilde{X}_t=w_t$ donde $\phi(B)= 1- {\phi}_1 B$.

- Para obtener su representación $MA(\infty)$, tenemos que 
$$\left(1- \phi_1 B \right) \left(1+ \psi_1 B+ \psi_2 B^2+... \right)=1$$
- Abriendo la expresión:

$$\begin{split}
\left(1- \phi_1 B \right) + \left(1- \phi_1 B \right) \psi_1 B + \left(1- \phi_1 B \right) \psi_2 B^2 \\
+ \left(1- \phi_1 B \right) \psi_3 B^3  +...  = 1 \end{split}$$

$$\begin{split} \Rightarrow 1- \phi_1 B + \psi_1 B- \phi_1 \psi_1 B^2   + \psi_2 B^2- \phi_1 \psi_2 B^3 \\ +\psi_3 B^3- \phi_1 \psi_3 B^4 + ...   = 1 \end{split}$$
$$\Rightarrow 1+ (\psi_1-\phi_1) B+ (\psi_2- \phi_1 \psi_1) B^2 + (\psi_3- \phi_1 \psi_2) B^3 + ...   = 1$$

 
---

- lo cual significa que:

$$\left. \begin{eqnarray} & \psi_1-\phi_1 & = & 0 \\ & \psi_2-\psi_1\phi_1 &= & 0 \\ & \psi_3-\psi_2\phi_1& = & 0 \\
&\vdots &=& \vdots \end{eqnarray}\right.$$

- La representación de $AR(1)$ como $MA(\infty)$ es:

$$\psi_1=\phi_1,~\psi_2=\phi_1^2,~\psi_3=\phi_1^3,~...$$

- O sea:

$$X_t-\mu=\sum_{j=0}^{\infty}\phi_1^j w_{t-j}.$$

# ARIMA(p,d,q)

## ARMA(p,q)

- El modelo lineal con representación AR y MA de orden infinito es elegante pero dificulta el proceso de la estimación involucrando infinitos coeficientes.
- El modelo de nuestro interés se centra en modelos que se pueden representar por medio de una cantidad finita de coeficientes.

- $X_t$ sigue un proceso ARMA(p,q) si satisface la siguiente ecuación:

$$\phi(B) \tilde{X}_t =\theta(B)a_t.$$
donde:<br /> 
$\tilde{X}_t = (X_t-\mu_X)$, es decir $\tilde{X}_t$ está centrada en cero. <br /> 
$\phi(B)=1-\phi_1 B -\phi_2  B^2-...-\phi_p B^p$ es el operador autorregresivo.<br /> 
$\theta(B)=1-\theta_1 B-\theta_2 B^2-...-\theta_q B^q$ es el operador de medias móviles.

## Condiciones de estacionariedad e invertibilidad de ARMA(p,q)

- La condición de estacionariedad se verifica con [la ecuación característica del proceso autoregresivo]{.alert} 
$$\phi(B)=1-\phi_1 B -\phi_2  B^2-...-\phi_p B^p=0.$$
- Si las raíces características de la ecuación característica están fuera del círculo unitario, el proceso AR(p) es estacionario.

- De forma similar, la condición de invertibilidad se verifica con la ecuación característica del proceso de medias móviles 

$$\theta(B)=1-\theta_1 B-\theta_2 B^2-...-\theta_q B^q=0.$$

- Si las raíces características de la ecuación característica están fuera del círculo unitario, el proceso MA(q) es invertible.


## ARIMA(p,d,q)

- En la práctica, muchas series son no estacionarias pero las diferencias consecutivas de orden $d$ puede llegar a una serie estacionaria.

- Sea $$\tilde{W}_t=\nabla^d \tilde{Z}_t=(1-B)^d \tilde{Z}_t$$ la diferencia consecutiva de orden $d$ de la serie $\tilde{Z}_t$.

- Suponga que después de realizar estas $d$ diferencias $\tilde{W}_t$ puede ser representado por un proceso ARMA(p,q), i.e.

$$(1-\phi_1 B -\phi_2  B^2-...-\phi_p B^p)\tilde{W}_t=(1-\theta_1 B-\theta_2 B^2-...-\theta_q B^q)a_t,$$
O equivalentemente,

$$(1-\phi_1 B -\phi_2  B^2-...-\phi_p B^p)(1-B)^d \tilde{Z}_t=$$
$$(1-\theta_1 B-\theta_2 B^2-...-\theta_q B^q)a_t.$$

## ARIMA(p,d,q)

- Usando las notaciones del operador de diferencia, autoregresivo $\phi(B)$ y de medias móviles $\theta(B)$, sea $$\tilde{W}_t=\nabla^d \tilde{Z}_t=(1-B)^d \tilde{Z}_t$$ la diferencia consecutiva de orden $d$ de la serie $\tilde{Z}_t$.

$$\phi(B)\nabla^d \tilde{Z}_t=\theta(B)a_t.$$
donde:<br /> 
$\phi(B)=1-\phi_1 B -\phi_2  B^2-...-\phi_p B^p$ es el operador autorregresivo.<br /> 
$\theta(B)=1-\theta_1 B-\theta_2 B^2-...-\theta_q B^q$ es el operador de medias móviles.
$\nabla^d=(1-B)^d$ es el operador de $d$ diferencias.

- Este modelo es denominado modelo autoregresivo integrado de promedios móviles (en inglés: AutoRegressive Integrated Moving Average model).

- Se denota con ARIMA(p,d,q).

## ARIMA(p,d,q)

- El término "integrado" proviene del hecho de que cuando $d=1$, $Z_t$ se puede presentar como la suma:
$$Z_t=W_t+W_{t-1}+W_{t-2}+...,$$
i.e. obtener $Z_t$ sumando (integrando) del proceso estacionario $W_t$.

$$W_t+W_{t-1}+W_{t-2}+...$$
$$=(Z_{t}-Z_{t-1})+(Z_{t-1}-Z_{t-2})+(Z_{t-2}-Z_{t-3})+...$$
$$=Z_{t}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$$

# En R


## Ejemplo 1 {.scrollable}

```{r}
#| echo: true
m<-5 #la media del proceso
y1 <- arima.sim(n = 150, model = list(order = c(1,0,0),ar = c(0.8)),sd=3,rand.gen= rnorm) + m
ts.plot(y1)
acf2(y1)

mod1<- forecast::Arima(y1, order = c(1, 0, 0))
summary(mod1)

```


## Modelo lineal {.scrollable}

```{r}
#| echo: true
#ARMA(2,1)
phi=c(0.4,0.2)
theta=c(-0.6)

#Representación en MA
ARMAtoMA(ar=phi, ma=theta,lag.max=5)
#Representación en AR
ARMAtoAR(ar = phi, ma = theta, lag.max=5)

#Observen que convergen a cero!
plot(ARMAtoMA(ar=phi, ma=theta,lag.max=20))
plot(ARMAtoAR(ar=phi, ma=theta,lag.max=20))
```


## En la próxima clase veremos

**Análisis espectral.**

## Paquetes en R

Para replicar los ejemplos de esta presentación, necesitan estos paquetes:

```{r}
#| echo: true
library(plotly)
library(astsa)
```





